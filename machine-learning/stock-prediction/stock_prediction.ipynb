{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameters\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 70\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 400\n",
    "\n",
    "# Apple stock market\n",
    "ticker = \"AAPL\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the data, default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    \n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/7845 [============================>.] - ETA: 0s - loss: 1.0049e-04 - mean_absolute_error: 0.0085\nEpoch 00342: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 1.0049e-04 - mean_absolute_error: 0.0085 - val_loss: 3.6804e-05 - val_mean_absolute_error: 0.0048\nEpoch 343/400\n7744/7845 [============================>.] - ETA: 0s - loss: 1.0413e-04 - mean_absolute_error: 0.0083\nEpoch 00343: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 237us/sample - loss: 1.0436e-04 - mean_absolute_error: 0.0083 - val_loss: 3.5577e-05 - val_mean_absolute_error: 0.0051\nEpoch 344/400\n7616/7845 [============================>.] - ETA: 0s - loss: 1.0095e-04 - mean_absolute_error: 0.0084\nEpoch 00344: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 249us/sample - loss: 1.0200e-04 - mean_absolute_error: 0.0084 - val_loss: 4.6649e-05 - val_mean_absolute_error: 0.0062\nEpoch 345/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.8952e-05 - mean_absolute_error: 0.0084\nEpoch 00345: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.8821e-05 - mean_absolute_error: 0.0084 - val_loss: 4.7311e-05 - val_mean_absolute_error: 0.0061\nEpoch 346/400\n7808/7845 [============================>.] - ETA: 0s - loss: 1.0634e-04 - mean_absolute_error: 0.0085\nEpoch 00346: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 237us/sample - loss: 1.0601e-04 - mean_absolute_error: 0.0085 - val_loss: 3.7422e-05 - val_mean_absolute_error: 0.0049\nEpoch 347/400\n7744/7845 [============================>.] - ETA: 0s - loss: 1.0303e-04 - mean_absolute_error: 0.0086\nEpoch 00347: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 1.0328e-04 - mean_absolute_error: 0.0086 - val_loss: 5.2242e-05 - val_mean_absolute_error: 0.0054\nEpoch 348/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.5551e-05 - mean_absolute_error: 0.0082\nEpoch 00348: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.4689e-05 - mean_absolute_error: 0.0082 - val_loss: 2.8914e-05 - val_mean_absolute_error: 0.0051\nEpoch 349/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.5323e-05 - mean_absolute_error: 0.0082\nEpoch 00349: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 237us/sample - loss: 9.5218e-05 - mean_absolute_error: 0.0082 - val_loss: 3.4655e-05 - val_mean_absolute_error: 0.0044\nEpoch 350/400\n7680/7845 [============================>.] - ETA: 0s - loss: 1.0109e-04 - mean_absolute_error: 0.0084\nEpoch 00350: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 1.0236e-04 - mean_absolute_error: 0.0084 - val_loss: 2.9309e-05 - val_mean_absolute_error: 0.0046\nEpoch 351/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.2117e-05 - mean_absolute_error: 0.0082\nEpoch 00351: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 246us/sample - loss: 9.2067e-05 - mean_absolute_error: 0.0082 - val_loss: 2.7584e-05 - val_mean_absolute_error: 0.0042\nEpoch 352/400\n7744/7845 [============================>.] - ETA: 0s - loss: 1.1017e-04 - mean_absolute_error: 0.0086\nEpoch 00352: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 1.0960e-04 - mean_absolute_error: 0.0085 - val_loss: 3.8670e-05 - val_mean_absolute_error: 0.0046\nEpoch 353/400\n7616/7845 [============================>.] - ETA: 0s - loss: 9.5023e-05 - mean_absolute_error: 0.0081\nEpoch 00353: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 243us/sample - loss: 9.4544e-05 - mean_absolute_error: 0.0081 - val_loss: 2.7894e-05 - val_mean_absolute_error: 0.0041\nEpoch 354/400\n7680/7845 [============================>.] - ETA: 0s - loss: 1.0254e-04 - mean_absolute_error: 0.0082\nEpoch 00354: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 245us/sample - loss: 1.0216e-04 - mean_absolute_error: 0.0082 - val_loss: 4.0644e-05 - val_mean_absolute_error: 0.0045\nEpoch 355/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.9838e-05 - mean_absolute_error: 0.0083\nEpoch 00355: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.9719e-05 - mean_absolute_error: 0.0083 - val_loss: 3.0785e-05 - val_mean_absolute_error: 0.0038\nEpoch 356/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.1214e-05 - mean_absolute_error: 0.0081\nEpoch 00356: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 242us/sample - loss: 9.1386e-05 - mean_absolute_error: 0.0081 - val_loss: 3.2558e-05 - val_mean_absolute_error: 0.0049\nEpoch 357/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.3233e-05 - mean_absolute_error: 0.0081\nEpoch 00357: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.2841e-05 - mean_absolute_error: 0.0081 - val_loss: 2.8253e-05 - val_mean_absolute_error: 0.0038\nEpoch 358/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.7001e-05 - mean_absolute_error: 0.0082\nEpoch 00358: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.6795e-05 - mean_absolute_error: 0.0082 - val_loss: 3.6283e-05 - val_mean_absolute_error: 0.0052\nEpoch 359/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.4018e-05 - mean_absolute_error: 0.0082\nEpoch 00359: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.3742e-05 - mean_absolute_error: 0.0082 - val_loss: 3.8966e-05 - val_mean_absolute_error: 0.0045\nEpoch 360/400\n7808/7845 [============================>.] - ETA: 0s - loss: 1.1265e-04 - mean_absolute_error: 0.0086\nEpoch 00360: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 255us/sample - loss: 1.1252e-04 - mean_absolute_error: 0.0086 - val_loss: 4.2502e-05 - val_mean_absolute_error: 0.0062\nEpoch 361/400\n7808/7845 [============================>.] - ETA: 0s - loss: 1.0046e-04 - mean_absolute_error: 0.0083\nEpoch 00361: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 1.0045e-04 - mean_absolute_error: 0.0083 - val_loss: 3.1767e-05 - val_mean_absolute_error: 0.0038\nEpoch 362/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.6965e-05 - mean_absolute_error: 0.0082\nEpoch 00362: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 239us/sample - loss: 9.6211e-05 - mean_absolute_error: 0.0082 - val_loss: 3.2622e-05 - val_mean_absolute_error: 0.0051\nEpoch 363/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.9916e-05 - mean_absolute_error: 0.0084\nEpoch 00363: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 268us/sample - loss: 9.8830e-05 - mean_absolute_error: 0.0084 - val_loss: 4.9724e-05 - val_mean_absolute_error: 0.0042\nEpoch 364/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.2970e-05 - mean_absolute_error: 0.0082\nEpoch 00364: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 9.3227e-05 - mean_absolute_error: 0.0083 - val_loss: 3.9159e-05 - val_mean_absolute_error: 0.0051\nEpoch 365/400\n7616/7845 [============================>.] - ETA: 0s - loss: 9.4369e-05 - mean_absolute_error: 0.0081\nEpoch 00365: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 237us/sample - loss: 9.4021e-05 - mean_absolute_error: 0.0081 - val_loss: 5.3123e-05 - val_mean_absolute_error: 0.0055\nEpoch 366/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.6416e-05 - mean_absolute_error: 0.0083\nEpoch 00366: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 9.5344e-05 - mean_absolute_error: 0.0082 - val_loss: 3.9583e-05 - val_mean_absolute_error: 0.0042\nEpoch 367/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.3304e-05 - mean_absolute_error: 0.0080\nEpoch 00367: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.4024e-05 - mean_absolute_error: 0.0080 - val_loss: 3.3550e-05 - val_mean_absolute_error: 0.0045\nEpoch 368/400\n7680/7845 [============================>.] - ETA: 0s - loss: 1.0465e-04 - mean_absolute_error: 0.0084\nEpoch 00368: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 243us/sample - loss: 1.0483e-04 - mean_absolute_error: 0.0084 - val_loss: 3.9235e-05 - val_mean_absolute_error: 0.0043\nEpoch 369/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.8507e-05 - mean_absolute_error: 0.0084\nEpoch 00369: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.7897e-05 - mean_absolute_error: 0.0084 - val_loss: 3.6304e-05 - val_mean_absolute_error: 0.0051\nEpoch 370/400\n7744/7845 [============================>.] - ETA: 0s - loss: 8.8661e-05 - mean_absolute_error: 0.0080\nEpoch 00370: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 8.8468e-05 - mean_absolute_error: 0.0080 - val_loss: 3.6597e-05 - val_mean_absolute_error: 0.0042\nEpoch 371/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.3120e-05 - mean_absolute_error: 0.0081\nEpoch 00371: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.4337e-05 - mean_absolute_error: 0.0081 - val_loss: 4.1640e-05 - val_mean_absolute_error: 0.0053\nEpoch 372/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.0803e-05 - mean_absolute_error: 0.0081\nEpoch 00372: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 9.0883e-05 - mean_absolute_error: 0.0081 - val_loss: 3.1984e-05 - val_mean_absolute_error: 0.0040\nEpoch 373/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.8615e-05 - mean_absolute_error: 0.0082\nEpoch 00373: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 243us/sample - loss: 9.8467e-05 - mean_absolute_error: 0.0082 - val_loss: 2.6773e-05 - val_mean_absolute_error: 0.0037\nEpoch 374/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.6735e-05 - mean_absolute_error: 0.0082\nEpoch 00374: val_loss improved from 0.00003 to 0.00003, saving model to results\\2020-04-28_AAPL-huber_loss-adam-LSTM-seq-100-step-20-layers-3-units-256.h5\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.6666e-05 - mean_absolute_error: 0.0082 - val_loss: 2.5108e-05 - val_mean_absolute_error: 0.0039\nEpoch 375/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.2388e-05 - mean_absolute_error: 0.0080\nEpoch 00375: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 243us/sample - loss: 9.3814e-05 - mean_absolute_error: 0.0081 - val_loss: 2.8615e-05 - val_mean_absolute_error: 0.0041\nEpoch 376/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.0247e-05 - mean_absolute_error: 0.0081\nEpoch 00376: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 250us/sample - loss: 9.0134e-05 - mean_absolute_error: 0.0081 - val_loss: 3.0442e-05 - val_mean_absolute_error: 0.0052\nEpoch 377/400\n7744/7845 [============================>.] - ETA: 0s - loss: 1.0059e-04 - mean_absolute_error: 0.0084\nEpoch 00377: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.9885e-05 - mean_absolute_error: 0.0083 - val_loss: 3.1755e-05 - val_mean_absolute_error: 0.0039\nEpoch 378/400\n7680/7845 [============================>.] - ETA: 0s - loss: 8.6334e-05 - mean_absolute_error: 0.0080\nEpoch 00378: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 242us/sample - loss: 8.6847e-05 - mean_absolute_error: 0.0080 - val_loss: 3.0288e-05 - val_mean_absolute_error: 0.0040\nEpoch 379/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.8619e-05 - mean_absolute_error: 0.0082\nEpoch 00379: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.8534e-05 - mean_absolute_error: 0.0082 - val_loss: 5.3537e-05 - val_mean_absolute_error: 0.0046\nEpoch 380/400\n7744/7845 [============================>.] - ETA: 0s - loss: 8.9094e-05 - mean_absolute_error: 0.0081\nEpoch 00380: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 8.8696e-05 - mean_absolute_error: 0.0081 - val_loss: 2.8393e-05 - val_mean_absolute_error: 0.0044\nEpoch 381/400\n7744/7845 [============================>.] - ETA: 0s - loss: 8.3325e-05 - mean_absolute_error: 0.0078\nEpoch 00381: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 242us/sample - loss: 8.3081e-05 - mean_absolute_error: 0.0078 - val_loss: 4.0752e-05 - val_mean_absolute_error: 0.0049\nEpoch 382/400\n7744/7845 [============================>.] - ETA: 0s - loss: 1.0078e-04 - mean_absolute_error: 0.0082\nEpoch 00382: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 1.0036e-04 - mean_absolute_error: 0.0082 - val_loss: 3.2613e-05 - val_mean_absolute_error: 0.0039\nEpoch 383/400\n7744/7845 [============================>.] - ETA: 0s - loss: 1.0006e-04 - mean_absolute_error: 0.0083\nEpoch 00383: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 243us/sample - loss: 1.0024e-04 - mean_absolute_error: 0.0083 - val_loss: 2.7228e-05 - val_mean_absolute_error: 0.0044\nEpoch 384/400\n7808/7845 [============================>.] - ETA: 0s - loss: 1.0005e-04 - mean_absolute_error: 0.0083\nEpoch 00384: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 242us/sample - loss: 9.9863e-05 - mean_absolute_error: 0.0083 - val_loss: 2.7866e-05 - val_mean_absolute_error: 0.0044\nEpoch 385/400\n7744/7845 [============================>.] - ETA: 0s - loss: 8.9173e-05 - mean_absolute_error: 0.0081\nEpoch 00385: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 8.9287e-05 - mean_absolute_error: 0.0081 - val_loss: 3.0670e-05 - val_mean_absolute_error: 0.0041\nEpoch 386/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.2664e-05 - mean_absolute_error: 0.0082\nEpoch 00386: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 9.2723e-05 - mean_absolute_error: 0.0082 - val_loss: 3.0496e-05 - val_mean_absolute_error: 0.0040\nEpoch 387/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.3552e-05 - mean_absolute_error: 0.0081\nEpoch 00387: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.3470e-05 - mean_absolute_error: 0.0081 - val_loss: 2.6446e-05 - val_mean_absolute_error: 0.0037\nEpoch 388/400\n7616/7845 [============================>.] - ETA: 0s - loss: 9.5983e-05 - mean_absolute_error: 0.0081\nEpoch 00388: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 243us/sample - loss: 9.6157e-05 - mean_absolute_error: 0.0081 - val_loss: 4.2435e-05 - val_mean_absolute_error: 0.0046\nEpoch 389/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.9344e-05 - mean_absolute_error: 0.0082\nEpoch 00389: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 9.8624e-05 - mean_absolute_error: 0.0082 - val_loss: 4.3023e-05 - val_mean_absolute_error: 0.0054\nEpoch 390/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.3577e-05 - mean_absolute_error: 0.0080\nEpoch 00390: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 240us/sample - loss: 9.3351e-05 - mean_absolute_error: 0.0080 - val_loss: 2.9023e-05 - val_mean_absolute_error: 0.0042\nEpoch 391/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.5114e-05 - mean_absolute_error: 0.0080\nEpoch 00391: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 250us/sample - loss: 9.4887e-05 - mean_absolute_error: 0.0080 - val_loss: 2.7452e-05 - val_mean_absolute_error: 0.0035\nEpoch 392/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.9411e-05 - mean_absolute_error: 0.0082\nEpoch 00392: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 242us/sample - loss: 9.9069e-05 - mean_absolute_error: 0.0082 - val_loss: 2.7251e-05 - val_mean_absolute_error: 0.0036\nEpoch 393/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.1213e-05 - mean_absolute_error: 0.0080\nEpoch 00393: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 238us/sample - loss: 9.1539e-05 - mean_absolute_error: 0.0080 - val_loss: 4.4506e-05 - val_mean_absolute_error: 0.0050\nEpoch 394/400\n7680/7845 [============================>.] - ETA: 0s - loss: 9.1334e-05 - mean_absolute_error: 0.0082\nEpoch 00394: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 9.0979e-05 - mean_absolute_error: 0.0081 - val_loss: 3.4280e-05 - val_mean_absolute_error: 0.0043\nEpoch 395/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.5261e-05 - mean_absolute_error: 0.0082\nEpoch 00395: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 248us/sample - loss: 9.4580e-05 - mean_absolute_error: 0.0082 - val_loss: 2.7989e-05 - val_mean_absolute_error: 0.0038\nEpoch 396/400\n7744/7845 [============================>.] - ETA: 0s - loss: 9.9826e-05 - mean_absolute_error: 0.0082\nEpoch 00396: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 239us/sample - loss: 9.9862e-05 - mean_absolute_error: 0.0082 - val_loss: 3.0989e-05 - val_mean_absolute_error: 0.0046\nEpoch 397/400\n7808/7845 [============================>.] - ETA: 0s - loss: 8.5800e-05 - mean_absolute_error: 0.0079\nEpoch 00397: val_loss did not improve from 0.00003\n7845/7845 [==============================] - 2s 241us/sample - loss: 8.5866e-05 - mean_absolute_error: 0.0079 - val_loss: 3.1226e-05 - val_mean_absolute_error: 0.0041\nEpoch 398/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.8951e-05 - mean_absolute_error: 0.0082\nEpoch 00398: val_loss improved from 0.00003 to 0.00002, saving model to results\\2020-04-28_AAPL-huber_loss-adam-LSTM-seq-100-step-20-layers-3-units-256.h5\n7845/7845 [==============================] - 2s 248us/sample - loss: 9.8815e-05 - mean_absolute_error: 0.0082 - val_loss: 2.3405e-05 - val_mean_absolute_error: 0.0035\nEpoch 399/400\n7680/7845 [============================>.] - ETA: 0s - loss: 8.6476e-05 - mean_absolute_error: 0.0078\nEpoch 00399: val_loss improved from 0.00002 to 0.00002, saving model to results\\2020-04-28_AAPL-huber_loss-adam-LSTM-seq-100-step-20-layers-3-units-256.h5\n7845/7845 [==============================] - 3s 320us/sample - loss: 8.6031e-05 - mean_absolute_error: 0.0078 - val_loss: 2.2425e-05 - val_mean_absolute_error: 0.0040\nEpoch 400/400\n7808/7845 [============================>.] - ETA: 0s - loss: 9.2852e-05 - mean_absolute_error: 0.0081\nEpoch 00400: val_loss did not improve from 0.00002\n7845/7845 [==============================] - 3s 349us/sample - loss: 9.2576e-05 - mean_absolute_error: 0.0081 - val_loss: 3.0416e-05 - val_mean_absolute_error: 0.0037\n"
    }
   ],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    plt.plot(y_test[-200:], c='b')\n",
    "    plt.plot(y_pred[-200:], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_accuracy(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
    "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "def predict(model, data, classification=False):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
    "    # retrieve the column scalers\n",
    "    column_scaler = data[\"column_scaler\"]\n",
    "    # reshape the last sequence\n",
    "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the optimal model weights\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data with shuffle = False\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean Absolute Error: 3.5988295\nFuture price after 20 days is 266.33$\nAccuracy Score: 0.8208032955715757\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (http://matplotlib.org/) -->\r\n<svg height=\"265.69625pt\" version=\"1.1\" viewBox=\"0 0 392.465625 265.69625\" width=\"392.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 265.69625 \r\nL 392.465625 265.69625 \r\nL 392.465625 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 228.14 \r\nL 381.765625 228.14 \r\nL 381.765625 10.7 \r\nL 46.965625 10.7 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mfd80c17d8a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.183807\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-30\"/>\r\n      </defs>\r\n      <g transform=\"translate(59.002557 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"100.420445\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 25 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-35\"/>\r\n      </defs>\r\n      <g transform=\"translate(94.057945 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.657082\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(132.294582 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.89372\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 75 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-37\"/>\r\n      </defs>\r\n      <g transform=\"translate(170.53122 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"215.130358\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-31\"/>\r\n      </defs>\r\n      <g transform=\"translate(205.586608 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"253.366995\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 125 -->\r\n      <g transform=\"translate(243.823245 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"291.603633\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(282.059883 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"329.840271\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 175 -->\r\n      <g transform=\"translate(320.296521 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.076909\" xlink:href=\"#mfd80c17d8a\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(358.533159 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_10\">\r\n     <!-- Days -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 8.109375 \r\nL 31.59375 8.109375 \r\nQ 46.6875 8.109375 53.6875 14.9375 \r\nQ 60.6875 21.78125 60.6875 36.53125 \r\nQ 60.6875 51.171875 53.6875 57.984375 \r\nQ 46.6875 64.796875 31.59375 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 30.078125 72.90625 \r\nQ 51.265625 72.90625 61.171875 64.09375 \r\nQ 71.09375 55.28125 71.09375 36.53125 \r\nQ 71.09375 17.671875 61.125 8.828125 \r\nQ 51.171875 0 30.078125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-44\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-61\"/>\r\n      <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-79\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-73\"/>\r\n     </defs>\r\n     <g transform=\"translate(201.8875 256.416562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-44\"/>\r\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"138.28125\" xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"197.460938\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md2407b8a32\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"205.979679\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(20.878125 209.778898)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"176.82936\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 220 -->\r\n      <g transform=\"translate(20.878125 180.628579)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"147.679042\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 240 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-34\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 151.478261)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"118.528724\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 260 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-36\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 122.327943)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"89.378405\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 280 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-38\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 93.177624)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"60.228087\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-33\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 64.027306)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#md2407b8a32\" y=\"31.077769\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 320 -->\r\n      <g transform=\"translate(20.878125 34.876988)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- Price -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-72\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-63\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 131.704375)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"60.287109\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"101.400391\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"129.183594\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"184.164062\" xlink:href=\"#DejaVuSans-65\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p253613b40f)\" d=\"M 62.183807 201.113382 \r\nL 63.713272 202.138799 \r\nL 65.242738 203.799654 \r\nL 66.772203 200.463487 \r\nL 68.301669 204.897261 \r\nL 69.831134 198.210494 \r\nL 71.3606 195.870875 \r\nL 72.890065 196.11636 \r\nL 74.419531 198.499323 \r\nL 75.948996 197.459496 \r\nL 77.478462 194.65771 \r\nL 79.007927 195.957522 \r\nL 80.537393 189.805091 \r\nL 83.596324 202.832017 \r\nL 85.125789 218.256364 \r\nL 86.655255 212.970489 \r\nL 88.18472 210.024277 \r\nL 89.714186 203.684118 \r\nL 91.243651 206.105134 \r\nL 92.773117 206.844501 \r\nL 94.302583 194.536413 \r\nL 95.832048 203.553636 \r\nL 97.361514 205.017846 \r\nL 98.890979 198.117219 \r\nL 100.420445 192.53582 \r\nL 101.94991 192.52132 \r\nL 103.479376 189.215977 \r\nL 105.008841 189.476918 \r\nL 106.538307 203.713097 \r\nL 108.067772 198.13172 \r\nL 109.597238 201.509542 \r\nL 111.126703 199.523428 \r\nL 112.656169 194.478456 \r\nL 114.185634 194.869856 \r\nL 115.7151 199.277009 \r\nL 117.244565 194.21747 \r\nL 118.774031 188.288172 \r\nL 120.303496 188.317173 \r\nL 121.832962 186.997923 \r\nL 123.362427 183.330158 \r\nL 124.891893 173.341628 \r\nL 126.421358 174.066494 \r\nL 127.950824 180.358237 \r\nL 129.480289 178.691065 \r\nL 131.009755 177.531297 \r\nL 132.53922 174.530375 \r\nL 134.068686 177.154353 \r\nL 135.598151 181.836969 \r\nL 137.127617 180.401738 \r\nL 138.657082 181.909449 \r\nL 140.186548 177.052894 \r\nL 143.245479 180.256733 \r\nL 144.774944 172.790745 \r\nL 146.30441 171.891919 \r\nL 147.833875 180.053794 \r\nL 149.363341 177.357336 \r\nL 150.892806 168.383614 \r\nL 152.422272 168.311134 \r\nL 153.951737 172.16736 \r\nL 155.481203 168.354614 \r\nL 157.010668 163.918504 \r\nL 158.540134 155.046263 \r\nL 160.069599 155.539166 \r\nL 161.599065 156.33649 \r\nL 163.12853 157.713742 \r\nL 164.657996 156.394492 \r\nL 166.187461 154.756299 \r\nL 167.716927 148.8125 \r\nL 169.246392 149.609824 \r\nL 170.775858 144.941774 \r\nL 172.305323 144.36189 \r\nL 173.834789 140.012739 \r\nL 175.364255 136.431955 \r\nL 176.89372 144.782314 \r\nL 178.423186 144.825793 \r\nL 179.952651 136.852378 \r\nL 181.482117 126.617386 \r\nL 183.011582 124.181891 \r\nL 184.541048 124.718251 \r\nL 186.070513 124.558813 \r\nL 187.599979 120.254809 \r\nL 189.129444 119.222387 \r\nL 190.65891 116.227025 \r\nL 192.188375 116.576014 \r\nL 193.717841 112.926308 \r\nL 195.247306 115.587226 \r\nL 196.776772 111.050548 \r\nL 198.306237 109.102108 \r\nL 199.835703 110.27989 \r\nL 201.365168 114.787479 \r\nL 202.894634 116.503289 \r\nL 204.424099 116.837733 \r\nL 205.953565 110.163575 \r\nL 207.48303 113.188027 \r\nL 209.012496 108.026096 \r\nL 210.541961 108.883979 \r\nL 212.071427 113.377067 \r\nL 213.600892 120.225675 \r\nL 215.130358 116.895913 \r\nL 216.659823 111.312312 \r\nL 218.189289 103.852951 \r\nL 219.718754 109.363827 \r\nL 221.24822 107.095488 \r\nL 222.777685 103.765682 \r\nL 224.307151 102.762394 \r\nL 225.836616 97.396878 \r\nL 227.366082 90.54827 \r\nL 228.895547 89.748478 \r\nL 230.425013 90.72272 \r\nL 231.954478 90.315596 \r\nL 233.483944 91.158934 \r\nL 235.013409 84.528411 \r\nL 236.542875 84.135832 \r\nL 238.07234 75.934858 \r\nL 239.601806 76.094852 \r\nL 241.131271 73.593839 \r\nL 242.660737 70.496707 \r\nL 244.190202 60.754417 \r\nL 245.719668 65.000331 \r\nL 247.249133 61.55421 \r\nL 248.778599 63.604375 \r\nL 250.308064 56.624907 \r\nL 251.83753 47.260697 \r\nL 253.366995 46.242909 \r\nL 254.896461 36.602434 \r\nL 256.425927 42.825833 \r\nL 257.955392 44.774273 \r\nL 259.484858 39.103447 \r\nL 261.014323 34.028696 \r\nL 262.543789 37.169507 \r\nL 264.073254 35.526377 \r\nL 265.60272 33.301673 \r\nL 267.132185 34.639449 \r\nL 268.661651 48.249484 \r\nL 270.191116 35.540966 \r\nL 271.720582 25.871446 \r\nL 273.250047 26.554835 \r\nL 274.779513 47.435191 \r\nL 276.308978 48.671153 \r\nL 277.838444 33.854246 \r\nL 279.367909 30.073636 \r\nL 280.897375 24.606395 \r\nL 282.42684 31.034045 \r\nL 283.956306 28.818637 \r\nL 285.485771 31.646221 \r\nL 287.015237 20.583636 \r\nL 288.544702 23.979673 \r\nL 290.074168 23.863047 \r\nL 291.603633 32.535285 \r\nL 293.133099 25.801568 \r\nL 294.662564 30.640532 \r\nL 296.19203 41.207522 \r\nL 297.721495 62.880777 \r\nL 299.250961 77.601696 \r\nL 300.780426 70.940838 \r\nL 302.309892 98.823125 \r\nL 303.839357 99.056332 \r\nL 305.368823 61.962535 \r\nL 306.898288 75.794346 \r\nL 308.427754 56.234508 \r\nL 309.957219 70.54728 \r\nL 311.486685 76.217038 \r\nL 313.01615 109.535831 \r\nL 314.545616 81.595276 \r\nL 316.075081 96.039264 \r\nL 317.604547 135.683692 \r\nL 319.134012 92.337161 \r\nL 320.663478 144.457922 \r\nL 322.192943 128.935387 \r\nL 323.722409 137.957414 \r\nL 325.251874 140.712118 \r\nL 326.78134 163.361905 \r\nL 328.310805 170.460023 \r\nL 329.840271 137.651325 \r\nL 331.369736 139.633548 \r\nL 332.899202 120.802445 \r\nL 334.428667 136.397861 \r\nL 335.958133 126.093235 \r\nL 337.487599 126.851149 \r\nL 339.017064 146.352697 \r\nL 340.54653 140.493499 \r\nL 342.075995 145.623939 \r\nL 343.605461 114.928658 \r\nL 345.134926 119.359519 \r\nL 346.664392 109.681591 \r\nL 348.193857 106.883186 \r\nL 349.723323 99.216638 \r\nL 351.252788 79.102936 \r\nL 352.782254 82.921621 \r\nL 354.311719 79.62762 \r\nL 355.841185 85.297379 \r\nL 357.37065 93.85299 \r\nL 358.900116 106.329323 \r\nL 360.429581 95.062709 \r\nL 361.959047 96.622261 \r\nL 363.488512 85.049581 \r\nL 365.017978 84.75806 \r\nL 366.547443 87.26499 \r\nL 366.547443 87.26499 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p253613b40f)\" d=\"M 62.183807 191.881588 \r\nL 63.713272 192.266539 \r\nL 65.242738 193.264467 \r\nL 66.772203 192.996342 \r\nL 68.301669 192.499058 \r\nL 69.831134 192.203689 \r\nL 71.3606 192.261202 \r\nL 72.890065 193.239781 \r\nL 74.419531 193.635941 \r\nL 75.948996 194.208908 \r\nL 79.007927 198.728929 \r\nL 80.537393 199.343218 \r\nL 82.066858 198.469589 \r\nL 83.596324 199.347999 \r\nL 85.125789 200.07311 \r\nL 88.18472 199.78508 \r\nL 89.714186 200.96075 \r\nL 91.243651 202.786425 \r\nL 92.773117 203.919483 \r\nL 94.302583 204.680667 \r\nL 95.832048 205.096465 \r\nL 98.890979 205.109564 \r\nL 100.420445 204.898952 \r\nL 101.94991 204.016738 \r\nL 103.479376 202.947354 \r\nL 105.008841 202.297392 \r\nL 109.597238 199.557877 \r\nL 111.126703 197.785066 \r\nL 112.656169 195.544327 \r\nL 114.185634 194.402284 \r\nL 115.7151 193.100492 \r\nL 117.244565 190.317966 \r\nL 118.774031 185.121028 \r\nL 120.303496 182.825334 \r\nL 121.832962 182.111966 \r\nL 123.362427 181.786974 \r\nL 124.891893 181.062508 \r\nL 126.421358 180.797119 \r\nL 127.950824 180.877428 \r\nL 129.480289 180.63161 \r\nL 131.009755 179.371918 \r\nL 132.53922 177.911222 \r\nL 134.068686 176.088259 \r\nL 135.598151 174.667128 \r\nL 137.127617 174.869578 \r\nL 138.657082 174.051794 \r\nL 140.186548 173.450671 \r\nL 141.716013 172.981364 \r\nL 143.245479 172.977027 \r\nL 144.774944 172.763902 \r\nL 146.30441 172.890558 \r\nL 147.833875 171.834184 \r\nL 150.892806 167.657525 \r\nL 152.422272 166.181328 \r\nL 153.951737 165.124753 \r\nL 155.481203 162.13862 \r\nL 157.010668 159.416475 \r\nL 160.069599 158.749834 \r\nL 161.599065 157.305929 \r\nL 163.12853 156.072636 \r\nL 164.657996 155.021132 \r\nL 166.187461 154.291484 \r\nL 169.246392 153.326716 \r\nL 170.775858 152.30517 \r\nL 172.305323 152.079657 \r\nL 173.834789 151.362708 \r\nL 175.364255 149.641293 \r\nL 176.89372 147.7098 \r\nL 178.423186 148.357649 \r\nL 179.952651 148.150684 \r\nL 181.482117 145.813891 \r\nL 183.011582 144.211326 \r\nL 184.541048 143.88242 \r\nL 186.070513 141.299363 \r\nL 187.599979 137.642274 \r\nL 189.129444 135.482421 \r\nL 190.65891 133.543833 \r\nL 192.188375 130.982772 \r\nL 195.247306 128.26463 \r\nL 199.835703 125.393478 \r\nL 201.365168 124.685959 \r\nL 202.894634 124.756904 \r\nL 204.424099 124.591573 \r\nL 205.953565 123.776546 \r\nL 207.48303 122.732248 \r\nL 210.541961 122.638796 \r\nL 213.600892 120.702365 \r\nL 216.659823 119.505146 \r\nL 218.189289 118.806056 \r\nL 219.718754 117.377318 \r\nL 221.24822 115.620142 \r\nL 222.777685 114.360917 \r\nL 224.307151 114.056452 \r\nL 225.836616 113.469496 \r\nL 227.366082 113.121263 \r\nL 228.895547 111.317249 \r\nL 230.425013 109.219223 \r\nL 231.954478 108.038194 \r\nL 235.013409 105.070676 \r\nL 238.07234 99.871782 \r\nL 239.601806 98.669625 \r\nL 241.131271 98.197115 \r\nL 242.660737 94.362329 \r\nL 244.190202 88.982579 \r\nL 247.249133 83.218212 \r\nL 248.778599 79.611563 \r\nL 250.308064 75.036009 \r\nL 251.83753 70.862954 \r\nL 253.366995 68.434665 \r\nL 254.896461 66.61535 \r\nL 256.425927 65.121983 \r\nL 257.955392 63.421919 \r\nL 259.484858 61.218387 \r\nL 262.543789 58.098347 \r\nL 264.073254 57.533586 \r\nL 265.60272 57.464331 \r\nL 268.661651 55.693277 \r\nL 270.191116 54.074121 \r\nL 271.720582 53.10535 \r\nL 273.250047 52.62817 \r\nL 274.779513 52.752625 \r\nL 276.308978 52.39732 \r\nL 277.838444 52.417647 \r\nL 279.367909 52.011368 \r\nL 280.897375 52.315343 \r\nL 283.956306 51.40186 \r\nL 285.485771 50.592371 \r\nL 287.015237 50.249966 \r\nL 288.544702 50.063817 \r\nL 291.603633 50.731771 \r\nL 293.133099 51.480412 \r\nL 294.662564 52.668825 \r\nL 296.19203 54.461541 \r\nL 297.721495 56.435156 \r\nL 299.250961 60.430337 \r\nL 300.780426 64.76908 \r\nL 302.309892 68.058899 \r\nL 303.839357 72.715807 \r\nL 305.368823 76.036584 \r\nL 306.898288 80.158532 \r\nL 308.427754 81.742415 \r\nL 309.957219 85.105315 \r\nL 311.486685 87.662106 \r\nL 313.01615 92.810605 \r\nL 314.545616 97.073821 \r\nL 317.604547 105.073212 \r\nL 319.134012 109.163357 \r\nL 320.663478 116.917174 \r\nL 322.192943 126.5383 \r\nL 323.722409 129.862547 \r\nL 325.251874 131.664292 \r\nL 326.78134 134.751016 \r\nL 328.310805 143.36096 \r\nL 329.840271 142.711376 \r\nL 331.369736 140.898444 \r\nL 332.899202 138.350838 \r\nL 334.428667 134.105836 \r\nL 335.958133 132.197295 \r\nL 337.487599 129.227664 \r\nL 339.017064 128.756711 \r\nL 340.54653 127.381549 \r\nL 342.075995 125.570174 \r\nL 343.605461 121.666689 \r\nL 345.134926 119.775049 \r\nL 346.664392 118.102117 \r\nL 348.193857 114.461397 \r\nL 349.723323 112.739092 \r\nL 351.252788 109.077644 \r\nL 352.782254 106.454133 \r\nL 354.311719 104.461035 \r\nL 355.841185 103.250294 \r\nL 357.37065 101.729038 \r\nL 358.900116 99.238477 \r\nL 360.429581 99.274551 \r\nL 361.959047 100.171665 \r\nL 363.488512 102.329472 \r\nL 365.017978 106.308684 \r\nL 366.547443 107.739023 \r\nL 366.547443 107.739023 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 46.965625 228.14 \r\nL 46.965625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 381.765625 228.14 \r\nL 381.765625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 46.965625 228.14 \r\nL 381.765625 228.14 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 46.965625 10.7 \r\nL 381.765625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 53.965625 48.05625 \r\nL 161.048438 48.05625 \r\nQ 163.048438 48.05625 163.048438 46.05625 \r\nL 163.048438 17.7 \r\nQ 163.048438 15.7 161.048438 15.7 \r\nL 53.965625 15.7 \r\nQ 51.965625 15.7 51.965625 17.7 \r\nL 51.965625 46.05625 \r\nQ 51.965625 48.05625 53.965625 48.05625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 55.965625 23.798437 \r\nL 75.965625 23.798437 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_19\">\r\n     <!-- Actual Price -->\r\n     <defs>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-41\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-74\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-75\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-6c\"/>\r\n      <path id=\"DejaVuSans-20\"/>\r\n     </defs>\r\n     <g transform=\"translate(83.965625 27.298437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"68.392578\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"123.373047\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"162.582031\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"225.960938\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"287.240234\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"315.023438\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"346.810547\" xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"407.097656\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"448.210938\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"475.994141\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"530.974609\" xlink:href=\"#DejaVuSans-65\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 55.965625 38.476562 \r\nL 75.965625 38.476562 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\"/>\r\n    <g id=\"text_20\">\r\n     <!-- Predicted Price -->\r\n     <defs>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-64\"/>\r\n     </defs>\r\n     <g transform=\"translate(83.965625 41.976562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"60.287109\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"101.369141\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-64\"/>\r\n      <use x=\"226.369141\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"254.152344\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"309.132812\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"348.341797\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"409.865234\" xlink:href=\"#DejaVuSans-64\"/>\r\n      <use x=\"473.341797\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"505.128906\" xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"565.416016\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"606.529297\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"634.3125\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"689.292969\" xlink:href=\"#DejaVuSans-65\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p253613b40f\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.965625\" y=\"10.7\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# evaluate the model\n",
    "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform(mae.reshape(1, -1))[0][0]\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(\"Accuracy Score:\", get_accuracy(model, data))\n",
    "plot_graph(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36664bitea6884f10f474b21a2a2f022451e0d09",
   "display_name": "Python 3.6.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}